{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "## Student : Jad El Karchi - AI\n",
    "\n",
    "# Inroduction\n",
    "\n",
    "Useful links:\n",
    "- Intorduction to Quantization on PyTorch https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n",
    "- PyTorch modules that provide quantization classes and functions https://pytorch.org/docs/stable/quantization.html#modules-that-provide-quantization-functions-and-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 [2 points]\n",
    "\n",
    "For X = DD + MM + YY + YY, where DD/MM/YYYY is your date of birth  \n",
    "\n",
    "1. Represent the number X in the following formats:\n",
    "\n",
    "  a) int8,  big-endian\n",
    "\n",
    "  b) int8, little-endian\n",
    "\n",
    "  b) int16, big-endian\n",
    "\n",
    "  c) int16, little-endian\n",
    "\n",
    "  d) float32, big-endian\n",
    "\n",
    "  e) float32, little-endian\n",
    "\n",
    "2. Write representations in the same formats a)-e) for -X\n",
    "\n",
    "\n",
    "Use the following style when writing/printing:\n",
    "\n",
    "- For all cases: 8 bits - space - 8 bits - space - ...\n",
    "\n",
    "- Additionally, for float big-endian format print the result the following ways: \n",
    "\n",
    "  - sign bit - space - exponent bits - space - fraction bits\n",
    "\n",
    "  - sign multiplier - space - exponent multiplier - space - faction multiplier (all multipliers in float format, multiply them to check whether the result is close to the initial number)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DD, MM, YY_1, YY_2 = 18, 12, 20, 1\n",
    "\n",
    "X = DD + MM + YY_1 + YY_2\n",
    "\n",
    "neg_X = -X\n",
    "\n",
    "\n",
    "def represent_in_formats(value, data_type, endian):\n",
    "    try :\n",
    "        if data_type == 'int8':\n",
    "            packed_value = value.to_bytes(1, byteorder=endian, signed=True)\n",
    "        elif data_type == 'int16':\n",
    "            packed_value = value.to_bytes(2, byteorder=endian, signed=True)\n",
    "        elif data_type == 'float32':\n",
    "            packed_value = value.to_bytes(4, byteorder=endian, signed=True)\n",
    "        else:\n",
    "            return \"Unsupported data type\"\n",
    "        \n",
    "        binary_representation = ' '.join(format(byte, '08b') for byte in packed_value)\n",
    "        return binary_representation\n",
    "    except :\n",
    "        return \"value unsupported (too big)\"\n",
    "\n",
    "    \n",
    "formats = [('int8', 'big'), ('int8', 'little'), ('int16', 'big'), ('int16', 'little'), ('float32', 'big'), ('float32', 'little')]\n",
    "for data_type, endian in formats:\n",
    "    print(f\"{data_type}, {endian}-endian:\")\n",
    "    representation_X = represent_in_formats(X, data_type, endian)\n",
    "    representation_neg_X = represent_in_formats(neg_X, data_type, endian)\n",
    "\n",
    "    print(f\"  X: {representation_X}\")\n",
    "    print(f\" -X: {representation_neg_X}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 [4 point]\n",
    "\n",
    "Given tensor X and using PyTorch tools\n",
    "\n",
    "\n",
    "1. Implement per-tensor affine quantization :\n",
    "\n",
    "  a) int8 symmetric \n",
    "\n",
    "  b) uint8 symmetric \n",
    "\n",
    "  c) int8 assymmetric\n",
    "\n",
    "  You'll need to do that for several given input tensors.\n",
    "  \n",
    "  - What can you say by comparing approximation errors of a)-c) representations? \n",
    "\n",
    "  - Explain why some quantization schemes suit better for some inputs.\n",
    "\n",
    "2. Implement per-tensor and per-channel (along axis = 0) affine quantization using int8 symmetric quantization.\n",
    "\n",
    "  You'll need to do that for several given input tensors.\n",
    "\n",
    "  - What can you say  by comparing approximation errors for per-tensor and per-channel quantization  for different inputs? \n",
    "\n",
    "  - Explain why some quantization schemes suit better for some inputs.\n",
    "\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- Quantized Tensors in PyTorch: https://pytorch.org/docs/stable/quantization.html#quantized-tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demostrative Example\n",
    "\n",
    "Pay attention to types of tensors when performing quantization / dequantization using PyTorch tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After generation: \n",
      "\t type: torch.float32, \n",
      "\n",
      "\t tesor: tensor([ 1.1300,  0.6253,  1.2075,  2.3684,  4.7084,  5.0779,  7.0216,  7.8060,\n",
      "         9.1087,  9.4054, 10.6627, 11.2264, 12.2299, 14.5136, 14.7325, 13.5463])\n",
      "\n",
      "\n",
      "After quantization: \n",
      "\t type: torch.qint8,\n",
      "\n",
      "\t quantized tensor in float representation: tensor([ 1.5000,  0.0000,  1.5000,  3.0000,  4.5000,  4.5000,  7.5000,  7.5000,\n",
      "         9.0000,  9.0000, 10.5000, 10.5000, 12.0000, 15.0000, 15.0000, 13.5000],\n",
      "       size=(16,), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=1.5, zero_point=0), \n",
      "\n",
      "\t quantized tensor in int representation: tensor([ 1,  0,  1,  2,  3,  3,  5,  5,  6,  6,  7,  7,  8, 10, 10,  9],\n",
      "       dtype=torch.int8)\n",
      "\n",
      "\n",
      "After dequantization: \n",
      "\t type: torch.float32, \n",
      "\n",
      "\t dequantized tesor: tensor([ 1.5000,  0.0000,  1.5000,  3.0000,  4.5000,  4.5000,  7.5000,  7.5000,\n",
      "         9.0000,  9.0000, 10.5000, 10.5000, 12.0000, 15.0000, 15.0000, 13.5000])\n"
     ]
    }
   ],
   "source": [
    "# Generate a float tensor\n",
    "t = torch.arange(512).reshape(32, 16) + torch.randn((32, 16))\n",
    "\n",
    "print(f'After generation: \\n\\t type: {t.dtype}, \\n\\n\\t tesor: {t[0, :]}')\n",
    "\n",
    "# Quantize the tensor\n",
    "qt = torch.quantize_per_tensor(t, scale=1.5, zero_point=0, dtype=torch.qint8)\n",
    "\n",
    "print(f'\\n\\nAfter quantization: \\n\\t type: {qt.dtype},'+\n",
    "      f'\\n\\n\\t quantized tensor in float representation: {qt[0, :]},'+\n",
    "      f' \\n\\n\\t quantized tensor in int representation: {qt.int_repr()[0, :]}')\n",
    "\n",
    "# Dequantize the tensor\n",
    "dqt = qt.dequantize()\n",
    "\n",
    "print(f'\\n\\nAfter dequantization: \\n\\t type: {dqt.dtype}, \\n\\n\\t dequantized tesor: {dqt[0, :]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 0\n"
     ]
    }
   ],
   "source": [
    "# Print scale and zero_point\n",
    "try:\n",
    "  print(qt.q_scale(), qt.q_zero_point())\n",
    "except:\n",
    "  print(qt.q_per_channel_scales(), qt.q_per_channel_zero_points())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.0 Implement Quantization / Dequantization [2 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(x, torch_dtype, is_symmetric=True, is_per_channel=False, axis = 0):\n",
    "  if is_per_channel:\n",
    "    x_quantized = quantize_tensor_per_channel(x, torch_dtype, is_symmetric, axis)\n",
    "\n",
    "  else:\n",
    "    x_quantized = quantize_tensor_per_tensor(x, torch_dtype, is_symmetric)\n",
    "\n",
    "  return x_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Tensor Quantization. Fill the blanks in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor_per_tensor(x, torch_dtype, is_symmetric=True):\n",
    "\n",
    "    print(f\"\\nQuantization type: {torch_dtype}, symmetric: {is_symmetric}, per_channel: {False} \")\n",
    "\n",
    "    bits = torch.iinfo(torch_dtype).bits\n",
    "\n",
    "    # Minimum  and maximum quantization values\n",
    "    if torch_dtype == torch.qint8: # torch.iinfo(torch_dtype).min != 0 \n",
    "      quant_min = -2**(bits - 1) \n",
    "      # ANSWER\n",
    "      quant_max =  2**(bits-1)-1\n",
    "\n",
    "    elif torch_dtype == torch.quint8: #\n",
    "      quant_min = 0 \n",
    "      # ANSWER\n",
    "      quant_max =  2**(bits)-1\n",
    "\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    \n",
    "\n",
    "    if is_symmetric:\n",
    "      scale = 2 * torch.where(x_min.abs() > x_max, x_min.abs(), x_max) / (quant_max - quant_min)\n",
    "      zero_point = 2**(bits - 1) if torch.iinfo(torch_dtype).min == 0 else 0\n",
    "    else:\n",
    "      x_max = max(x_max, 0)\n",
    "      x_min = min(x_min, 0)\n",
    "      # ANSWER\n",
    "      scale =  (x_max - x_min) / (quant_max - quant_min)\n",
    "      zero_point = quant_min-round(int(np.array(x_min)/scale.item()))\n",
    "\n",
    "\n",
    "    # Use PyTorch build-in function\n",
    "    x_quantized = torch.quantize_per_tensor(x, scale, zero_point=zero_point, dtype=torch_dtype)\n",
    "\n",
    "    return x_quantized                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: False \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Unit test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m qt \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_tensor_per_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_symmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(t \u001b[38;5;241m-\u001b[39m qt\u001b[38;5;241m.\u001b[39mdequantize()) \u001b[38;5;241m<\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(t\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m      6\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mquantize_tensor_per_tensor\u001b[0;34m(x, torch_dtype, is_symmetric)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m# ANSWER\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   scale \u001b[38;5;241m=\u001b[39m  (x_max \u001b[38;5;241m-\u001b[39m x_min) \u001b[38;5;241m/\u001b[39m (quant_max \u001b[38;5;241m-\u001b[39m quant_min)\n\u001b[0;32m---> 30\u001b[0m   zero_point \u001b[38;5;241m=\u001b[39m quant_min \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(x_min) \u001b[38;5;241m/\u001b[39m scale\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Use PyTorch build-in function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x_quantized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(x, scale, zero_point\u001b[38;5;241m=\u001b[39mzero_point, dtype\u001b[38;5;241m=\u001b[39mtorch_dtype)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "t = torch.ones(512).reshape(32, 16)\n",
    "qt = quantize_tensor_per_tensor(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps\n",
    "\n",
    "t = torch.ones(512).reshape(32, 16) + 1e-8\n",
    "qt = quantize_tensor_per_tensor(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps\n",
    "\n",
    "t = torch.arange(256).reshape(8, 32).to(torch.float32)\n",
    "qt = quantize_tensor_per_tensor(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Channel Quantization. Fill the blanks in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor_per_channel(x, torch_dtype, is_symmetric=True, axis = 0):\n",
    "  '''\n",
    "  Takes float PyTorch tensor as input.\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  x_quantized: PyTorch tensor of PyTorch quantization types (e.g., torch.qint8, torch.quint8)\n",
    "    This format contains integer representation, scale, zero_point.\n",
    "    If you want to extract integer representation use 'x_int = x_quantized.int_repr()'.\n",
    "  '''\n",
    "  \n",
    "  print(f\"\\nQuantization type: {torch_dtype}, symmetric: {is_symmetric}, per_channel: {True} \")\n",
    "\n",
    "  bits = torch.iinfo(torch_dtype).bits\n",
    "\n",
    "\n",
    "  if torch_dtype==torch.qint8:\n",
    "    quant_min = -2**(bits - 1)\n",
    "    quant_max =  2**(bits - 1) - 1\n",
    "\n",
    "  elif torch_dtype==torch.quint8:\n",
    "    quant_min = 0\n",
    "    quant_max =  2*(2**(bits-1))-1\n",
    "\n",
    "  unfolded_t = tl.base.unfold(x, mode=0)\n",
    "  x_max =  unfolded_t.max(dim=-1)[0]\n",
    "  x_min =  unfolded_t.min(dim=-1)[0]\n",
    "  \n",
    "\n",
    "\n",
    "  if is_symmetric:\n",
    "    scale = 2 * torch.where(x_min.abs() > x_max, x_min.abs(), x_max) / (quant_max - quant_min) \n",
    "    \n",
    "    zero_point = torch.repeat_interleave(\n",
    "        torch.tensor(2**(bits - 1) if torch.iinfo(torch_dtype).min == 0 else 0),len(scale))\n",
    "\n",
    "  else:\n",
    "    x_max = torch.where(x_max < torch.zeros_like(x_max), torch.zeros_like(x_max), x_max)\n",
    "    x_min = torch.where(x_min > torch.zeros_like(x_min), torch.zeros_like(x_min), x_min)\n",
    "\n",
    "    scale = ((x_max - x_min) / (quant_max - quant_min))\n",
    "    zero_point = torch.floor(quant_min-x_min/scale)\n",
    "\n",
    "    \n",
    "\n",
    "  # Use PyTorch build-in function\n",
    "  x_quantized =  torch.quantize_per_channel(x, scales=scale, zero_points=zero_point, dtype=torch_dtype, axis=axis)\n",
    "  return x_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: True \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: True \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: True \n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "t = torch.ones(512).reshape(32, 16).to(torch.float32)\n",
    "qt = quantize_tensor_per_channel(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps\n",
    "\n",
    "t = torch.ones(512).reshape(32, 16) + 1e-2\n",
    "qt = quantize_tensor_per_channel(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps\n",
    "\n",
    "t = torch.arange(256).reshape(1, 256).to(torch.float32)\n",
    "qt = quantize_tensor_per_channel(t, torch.qint8, is_symmetric=False)\n",
    "assert torch.max(t - qt.dequantize()) < torch.finfo(t.dtype).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dequantization. Fill blanks in the code. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize_tensor(x_quantized):\n",
    "    # Implement using scale, zero_point, integer representation\n",
    "    if x_quantized.qscheme() == torch.per_tensor_affine:\n",
    "        # Get scale, zero_point from x_quantized\n",
    "        # ANSWER\n",
    "        scale =  x_quantized.q_scale()\n",
    "        zero_point =  x_quantized.q_zero_point()\n",
    "\n",
    "    else:\n",
    "        axis = x_quantized.q_per_channel_axis() \n",
    "\n",
    "        # ANSWER \n",
    "        scale = x_quantized.q_per_channel_scales()\n",
    "        zero_point = x_quantized.q_per_channel_zero_points()\n",
    "\n",
    "        ## Broadcasting along axis:\n",
    "        for i, shp in enumerate(x_quantized.shape):\n",
    "            if i != axis:\n",
    "                scale.unsqueeze(i)\n",
    "                zero_point.unsqueeze(i)\n",
    "\n",
    "    # ANSWER\n",
    "    x_int =  x_quantized.int_repr()\n",
    "    x_dequantized =  scale * (x_int - zero_point).to(torch.long) # Use x_int in torch.long type to avoid errors\n",
    "\n",
    "    return x_dequantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "t = torch.arange(512).reshape(32, 16) + torch.randn((32, 16))\n",
    "\n",
    "# Unit test1 \n",
    "qt = torch.quantize_per_tensor(t, scale=1.5, zero_point=0, dtype=torch.qint8)\n",
    "dqt = qt.dequantize()\n",
    "\n",
    "dqt_custom = dequantize_tensor(qt)\n",
    "\n",
    "rel_err = torch.norm(dqt - dqt_custom) / torch.norm(dqt)\n",
    "assert rel_err < torch.finfo(t.dtype).eps, rel_err\n",
    "\n",
    "\n",
    "# Unit test2\n",
    "qt = torch.quantize_per_channel(t, scales=torch.arange(16).to(torch.float), zero_points=torch.zeros(16, dtype=torch.int), dtype=torch.qint8, axis=1)\n",
    "dqt = qt.dequantize()\n",
    "\n",
    "dqt_custom = dequantize_tensor(qt)\n",
    "\n",
    "rel_err = torch.norm(dqt - dqt_custom) / torch.norm(dqt)\n",
    "assert rel_err < torch.finfo(t.dtype).eps, rel_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1. Perform Quantization. Analyze. [1 point]\n",
    "\n",
    "For three given tensors perform per-tensor affine quantization :\n",
    "a) int8 symmetric\n",
    "b) uint8 symmetric\n",
    "c) int8 assymmetric\n",
    "\n",
    "\n",
    "- What can you say by comparing approximation errors of a)-c) representations?\n",
    "- Explain why some quantization schemes suit better for some inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.quint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.quint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.quint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: False, per_channel: False \n",
      "\n",
      "tensor number 0, tensor range: (tensor(0.0045), tensor(99.9478))\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_tensor'), approx_error: 0.003955337218940258\n",
      "\tquantization scheme (torch.quint8, 'symmetric', 'per_tensor'), approx_error: 0.003955337218940258\n",
      "\tquantization scheme (torch.qint8, 'asymmetric', 'per_tensor'), approx_error: 0.0019940254278481007\n",
      "\n",
      "tensor number 1, tensor range: (tensor(-49.9914), tensor(49.9931))\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_tensor'), approx_error: 0.003938150592148304\n",
      "\tquantization scheme (torch.quint8, 'symmetric', 'per_tensor'), approx_error: 0.003938150592148304\n",
      "\tquantization scheme (torch.qint8, 'asymmetric', 'per_tensor'), approx_error: 0.003938410896807909\n",
      "\n",
      "tensor number 2, tensor range: (tensor(-19.9893), tensor(79.9611))\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_tensor'), approx_error: 0.0043550110422074795\n",
      "\tquantization scheme (torch.quint8, 'symmetric', 'per_tensor'), approx_error: 0.0043550110422074795\n",
      "\tquantization scheme (torch.qint8, 'asymmetric', 'per_tensor'), approx_error: 0.0027127128560096025\n"
     ]
    }
   ],
   "source": [
    "shape = (512, 3, 3)\n",
    "x1 = torch.rand(shape) * 100      # uniform in the range [0, 100] \n",
    "x2 = torch.rand(shape) * 100 - 50 # uniform in the range [-50, 50] \n",
    "x3 = torch.rand(shape) * 100 - 20 # uniform in the range [-20, 80] \n",
    "\n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "\n",
    "# Experiments 1\n",
    "for torch_dtype, is_symmetric, is_per_channel in [[torch.qint8, True, False],\n",
    "                                                  [torch.quint8, True, False],\n",
    "                                                  [torch.qint8, False, False]]:\n",
    "\n",
    "  tmp_dict = defaultdict()\n",
    "\n",
    "  for i, x in enumerate([x1, x2, x3]):\n",
    "    x_quantized = quantize_tensor(x,\n",
    "                                  torch_dtype = torch_dtype,\n",
    "                                  is_symmetric = is_symmetric,\n",
    "                                  is_per_channel = is_per_channel)\n",
    "    \n",
    "    approx_error = torch.norm(x_quantized.dequantize() - x)/torch.norm(x)\n",
    "\n",
    "    tmp_dict[i] = (x_quantized, approx_error)\n",
    "\n",
    "  key = (torch_dtype,\n",
    "         'symmetric' if is_symmetric else 'asymmetric',\n",
    "         'per_channel' if is_per_channel else 'per_tensor')\n",
    "  results[key] = tmp_dict\n",
    "  \n",
    "\n",
    "# Compute difference in approximations\n",
    "for i, x in enumerate([x1, x2, x3]):\n",
    "  print(f\"\\ntensor number {i}, tensor range: {(x.min(), x.max())}\")\n",
    "  for key in results.keys():\n",
    "    print(f'\\tquantization scheme {key}, approx_error: {results[key][i][1]}')\n",
    "\n",
    "# TODO : COMMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a) int8 symmetric\n",
    "- Uses signed integers within a symmetric range.\n",
    "- Ideal for values centered around zero.\n",
    "\n",
    "b) uint8 symmetric\n",
    "- Uses unsigned integers within a symmetric range.\n",
    "- Like int8 symmetric but lacks negative values.\n",
    "\n",
    "c) int8 asymmetric\n",
    "- Uses signed integers with an asymmetric range.\n",
    "- Woeks best for values with a specific bias or offset from zero.\n",
    "\n",
    "\n",
    "The scheme depends on the value characteristics, Symmetric for zero-centered data, asymmetric for biased data. In this scenario, all schemes perform well, with int8 asymmetric showing an advantage for tensors with a mean different than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2. Perform Quantization. Analyze. [1 point]\n",
    "\n",
    "For two given tensors perform per-tensor and per-channel (along axis = 0) affine quantization using int8 symmetric quantization.\n",
    "\n",
    "- What can you say by comparing approximation errors for per-tensor and per-channel quantization for different inputs?\n",
    "- Explain why some quantization schemes suit better for some inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: False \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: True \n",
      "\n",
      "Quantization type: torch.qint8, symmetric: True, per_channel: True \n",
      "\n",
      "tensor number 0, tensor range: (tensor(0.0063), tensor(102393.9922))\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_tensor'), approx_error: 0.006783206947147846\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_channel'), approx_error: 0.003921858966350555\n",
      "\n",
      "tensor number 1, tensor range: (tensor(-50.0000), tensor(50.0000))\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_tensor'), approx_error: 0.00392519123852253\n",
      "\tquantization scheme (torch.qint8, 'symmetric', 'per_channel'), approx_error: 0.003924562595784664\n"
     ]
    }
   ],
   "source": [
    "shape = (1024, 32, 64)\n",
    "x1 = torch.rand(shape) * 100    # uniform in the range [0, 100] \n",
    "x1 = x1 * torch.arange(1, 1 + shape[0])[:, None, None] # then multiple each channel by (channel_index + 1)\n",
    "\n",
    "x2 = torch.rand(shape) * 100 - 50 # uniform in the range [-50, 50] \n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "\n",
    "# Experiments 2\n",
    "for torch_dtype, is_symmetric, is_per_channel in [[torch.qint8, True, False],\n",
    "                                                  [torch.qint8, True, True]]:\n",
    "\n",
    "  tmp_dict = defaultdict()\n",
    "\n",
    "  for i, x in enumerate([x1, x2]):\n",
    "    x_quantized = quantize_tensor(x,\n",
    "                                  torch_dtype = torch_dtype,\n",
    "                                  is_symmetric = is_symmetric,\n",
    "                                  is_per_channel = is_per_channel,\n",
    "                                  axis = 0)\n",
    "    \n",
    "    approx_error = torch.norm(x_quantized.dequantize() - x)/torch.norm(x)\n",
    "\n",
    "    tmp_dict[i] = (x_quantized, approx_error)\n",
    "\n",
    "  key = (torch_dtype,\n",
    "         'symmetric' if is_symmetric else 'asymmetric',\n",
    "         'per_channel' if is_per_channel else 'per_tensor')\n",
    "  results[key] = tmp_dict\n",
    "  \n",
    "\n",
    "# Compute difference in approximations\n",
    "for i, x in enumerate([x1, x2]):\n",
    "  print(f\"\\ntensor number {i}, tensor range: {(x.min(), x.max())}\")\n",
    "  for key in results.keys():\n",
    "    print(f'\\tquantization scheme {key}, approx_error: {results[key][i][1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 [4 points]\n",
    "\n",
    "Consider a simple PyTorch model and modify it using QuantStub() and DeQuantStub() to imitate\n",
    "\n",
    "1) Quantization of all layers\n",
    "\n",
    "2) Quantization only of convolutionsl layers\n",
    "\n",
    "Useful links:\n",
    "- PyTorch model preparation for Quantization https://pytorch.org/docs/stable/quantization.html#model-preparation-for-quantization\n",
    "\n",
    "\n",
    "Given a class for a neural network prepare it to be quantizable and quantize.\n",
    "\n",
    "Namely, you should quantize\n",
    "\n",
    "1. whole model using default PyTorch static quantization qconfig\n",
    "\n",
    "2. part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\") / 1024**2)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill blanks in `quantize_model` function that performs quantization [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, qconfig_dict={}, input_shape=None):\n",
    "\n",
    "    print(f'QConfig \\n {qconfig_dict}')\n",
    "\n",
    "    # Layers with qconfig=None will not be quantized\n",
    "    model.qconfig = None\n",
    "\n",
    "    for mname, m in model.named_modules():\n",
    "        if mname in qconfig_dict:\n",
    "            m.qconfig = qconfig_dict[mname]\n",
    "        \n",
    "    print(f'\\n Model before preparation \\n{model}')\n",
    "\n",
    "    # Prepare the model for quantization by propagating qconfig\n",
    "    \n",
    "    # ANSWER\n",
    "    model = torch.quantization.prepare(model)\n",
    "    \n",
    "    \n",
    "    print(f'\\n Model after preparation & before calibration (activation stats computation) \\n{model}')\n",
    "\n",
    "    # Collect statistics for quantization\n",
    "\n",
    "    # ANSWER\n",
    "    if input_shape is not None:\n",
    "        model(torch.randn(input_shape))\n",
    "    \n",
    "    print(f'\\n Model after calibration & before conversion \\n{model}')\n",
    "\n",
    "    # Quantize the model\n",
    "    \n",
    "    # ANSWER\n",
    "    model = torch.quantization.convert(model)\n",
    "\n",
    "    print(f'\\n Model after conversion \\n{model}')    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelkarchi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jelkarchi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "model = vgg16(pretrained=True)\n",
    "\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "  p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse all possible layers [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fmodel = copy.deepcopy(model)\n",
    "\n",
    "# Fuse all possible layers\n",
    "# ANSWER\n",
    "fused_model = torch.quantization.fuse_modules(\n",
    "    fmodel,\n",
    "    [\n",
    "        ['features.0', 'features.1'],  \n",
    "        ['features.2', 'features.3'],  \n",
    "        ['features.5', 'features.6'],  \n",
    "        ['features.7', 'features.8'],  \n",
    "        ['features.10', 'features.11'],\n",
    "        ['features.12', 'features.13'],\n",
    "        ['features.15', 'features.16'],\n",
    "        ['features.19', 'features.20'],\n",
    "        ['features.22', 'features.23'],\n",
    "        ['features.26', 'features.27'],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): ConvReLU2d(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): ConvReLU2d(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Identity()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ConvReLU2d(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Identity()\n",
       "    (7): ConvReLU2d(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Identity()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): ConvReLU2d(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Identity()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the whole model [1 point]\n",
    "\n",
    "\n",
    "1. whole model  using default PyTorch static quantization qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify model to be quantizable and set qconfig_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig \n",
      " QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "\n",
      " Model before preparation \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): ConvReLU2d(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Identity()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ConvReLU2d(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Identity()\n",
      "    (7): ConvReLU2d(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Identity()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ConvReLU2d(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Identity()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " Model after preparation & before calibration (activation stats computation) \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): ConvReLU2d(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Identity()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ConvReLU2d(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Identity()\n",
      "    (7): ConvReLU2d(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Identity()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ConvReLU2d(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Identity()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " Model after calibration & before conversion \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): ConvReLU2d(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Identity()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ConvReLU2d(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Identity()\n",
      "    (7): ConvReLU2d(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Identity()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ConvReLU2d(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Identity()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelkarchi/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:309: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model after conversion \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): ConvReLU2d(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Identity()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ConvReLU2d(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Identity()\n",
      "    (7): ConvReLU2d(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Identity()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ConvReLU2d(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Identity()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qmodel = copy.deepcopy(fused_model)\n",
    "# ANSWER\n",
    "qconfig_dict = torch.quantization.get_default_qconfig('fbgemm')\n",
    "qmodel = quantize_model(qmodel, qconfig_dict, input_shape=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 527.8007297515869\n",
      "Size (MB): 527.8010959625244\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model)\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check inference time\n",
    "shape = (2, 3, 228, 224)\n",
    "x = torch.randn(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _ = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _ = qmodel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize a  part of the model [1 point]\n",
    "\n",
    "Namely\n",
    "  - all convolutional layers using Min-Max observers \n",
    "  - first fully-connected layer using default PyTorch static quantization qconfig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = copy.deepcopy(fmodel)\n",
    "\n",
    "# Modify model to be quantizable and set qconfig_dict\n",
    "# ANSWER\n",
    "for layer in qmodel.features.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        qmodel.features[layer].qconfig = torch.quantization.default_observer\n",
    "        qmodel.features[layer] = torch.quantization.ObserverQuantize(layer)\n",
    "\n",
    "qmodel.classifier[0].qconfig = torch.quantization.default_qconfig\n",
    "qconfig_dict =  {\n",
    "    '': torch.quantization.default_qconfig,\n",
    "    'classifier.0': torch.quantization.default_qconfig,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model\n",
    "qmodel =  quantize_model(qmodel, qconfig_dict, input_shape=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size_of_model(model)\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check inference time\n",
    "shape = (2, 3, 228, 224)\n",
    "x = torch.randn(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _ = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _ = qmodel(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
